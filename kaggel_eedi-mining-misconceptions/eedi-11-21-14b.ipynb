{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24be853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:06.851851Z",
     "iopub.status.busy": "2024-12-12T10:40:06.851495Z",
     "iopub.status.idle": "2024-12-12T10:40:22.242033Z",
     "shell.execute_reply": "2024-12-12T10:40:22.241211Z"
    },
    "papermill": {
     "duration": 15.399579,
     "end_time": "2024-12-12T10:40:22.244107",
     "exception": false,
     "start_time": "2024-12-12T10:40:06.844528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/eedi-library-new\r\n",
      "Processing /kaggle/input/eedi-library-new/autoawq-0.2.7.post2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/eedi-library-new/peft-0.13.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (2.4.0)\r\n",
      "Processing /kaggle/input/eedi-library-new/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from autoawq==0.2.7.post2)\r\n",
      "Requirement already satisfied: transformers>=4.35.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (4.45.1)\r\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (4.12.2)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.34.2)\r\n",
      "Requirement already satisfied: datasets>=2.20 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (3.0.1)\r\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.23.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (4.66.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.15.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (2.32.3)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.20->autoawq==0.2.7.post2) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.9.5)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.13.2) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq==0.2.7.post2) (2024.5.15)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->autoawq==0.2.7.post2) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.2.0->autoawq==0.2.7.post2) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq==0.2.7.post2) (1.16.0)\r\n",
      "Installing collected packages: triton, peft, autoawq\r\n",
      "Successfully installed autoawq-0.2.7.post2 peft-0.13.2 triton-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/eedi-library-new/autoawq*.whl /kaggle/input/eedi-library-new/peft-0.13.2-py3-none-any.whl  --no-index --find-links=/kaggle/input/eedi-library-new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11fa2cb9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:22.257857Z",
     "iopub.status.busy": "2024-12-12T10:40:22.257552Z",
     "iopub.status.idle": "2024-12-12T10:40:22.265763Z",
     "shell.execute_reply": "2024-12-12T10:40:22.264953Z"
    },
    "papermill": {
     "duration": 0.016837,
     "end_time": "2024-12-12T10:40:22.267377",
     "exception": false,
     "start_time": "2024-12-12T10:40:22.250540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run1.py\n",
    "import os, math, numpy as np\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "pd.set_option('display.max_rows', 300)\n",
    "IS_SUBMISSION = True\n",
    "#bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "\n",
    "print('IS_SUBMISSION:', IS_SUBMISSION)\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "df_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "if not IS_SUBMISSION:\n",
    "    df_ret = df_train.copy()\n",
    "else:\n",
    "    df_ret = df_test.copy()\n",
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "            continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "LORA_PATH = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "import pickle\n",
    "with open('queries.pkl', 'wb') as f:\n",
    "    pickle.dump(new_queries, f)\n",
    "    \n",
    "with open('documents.pkl', 'wb') as f:\n",
    "    pickle.dump(documents, f)\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241a18a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:22.281391Z",
     "iopub.status.busy": "2024-12-12T10:40:22.281176Z",
     "iopub.status.idle": "2024-12-12T10:40:22.287398Z",
     "shell.execute_reply": "2024-12-12T10:40:22.286518Z"
    },
    "papermill": {
     "duration": 0.014822,
     "end_time": "2024-12-12T10:40:22.289002",
     "exception": false,
     "start_time": "2024-12-12T10:40:22.274180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 320\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c669c95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:22.301805Z",
     "iopub.status.busy": "2024-12-12T10:40:22.301549Z",
     "iopub.status.idle": "2024-12-12T10:40:41.024172Z",
     "shell.execute_reply": "2024-12-12T10:40:41.023104Z"
    },
    "papermill": {
     "duration": 18.731508,
     "end_time": "2024-12-12T10:40:41.026473",
     "exception": false,
     "start_time": "2024-12-12T10:40:22.294965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: True\r\n"
     ]
    }
   ],
   "source": [
    "!python run1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eef94af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:41.040359Z",
     "iopub.status.busy": "2024-12-12T10:40:41.039828Z",
     "iopub.status.idle": "2024-12-12T10:40:45.051587Z",
     "shell.execute_reply": "2024-12-12T10:40:45.050619Z"
    },
    "papermill": {
     "duration": 4.020922,
     "end_time": "2024-12-12T10:40:45.053709",
     "exception": false,
     "start_time": "2024-12-12T10:40:41.032787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sleep 1 & sleep 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50fb5680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:40:45.069045Z",
     "iopub.status.busy": "2024-12-12T10:40:45.068779Z",
     "iopub.status.idle": "2024-12-12T10:46:15.494314Z",
     "shell.execute_reply": "2024-12-12T10:46:15.493455Z"
    },
    "papermill": {
     "duration": 330.435895,
     "end_time": "2024-12-12T10:46:15.496656",
     "exception": false,
     "start_time": "2024-12-12T10:40:45.060761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:56<00:00, 58.36s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:57<00:00, 58.54s/it]\n",
      "Embedding: 100%|██████████| 325/325 [02:58<00:00,  1.82it/s]\n",
      "Embedding:  92%|█████████▏| 300/325 [02:59<00:16,  1.49it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile run.sh\n",
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adb1b75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:15.553331Z",
     "iopub.status.busy": "2024-12-12T10:46:15.553051Z",
     "iopub.status.idle": "2024-12-12T10:46:33.106876Z",
     "shell.execute_reply": "2024-12-12T10:46:33.105738Z"
    },
    "papermill": {
     "duration": 17.580278,
     "end_time": "2024-12-12T10:46:33.109155",
     "exception": false,
     "start_time": "2024-12-12T10:46:15.528877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 325/325 [03:13<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.pt.fold.1.2.embed\n",
      "data.pt.fold.0.2.embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/2689911440.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_to_embed.update(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "import torch\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ec6143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:33.159942Z",
     "iopub.status.busy": "2024-12-12T10:46:33.159530Z",
     "iopub.status.idle": "2024-12-12T10:46:33.164642Z",
     "shell.execute_reply": "2024-12-12T10:46:33.164097Z"
    },
    "papermill": {
     "duration": 0.032271,
     "end_time": "2024-12-12T10:46:33.166178",
     "exception": false,
     "start_time": "2024-12-12T10:46:33.133907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/kaggle/working/queries.pkl', 'rb') as f:\n",
    "    queries=pickle.load(f)\n",
    "with open('/kaggle/working/documents.pkl', 'rb') as f:\n",
    "    documents=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882f65a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:33.215503Z",
     "iopub.status.busy": "2024-12-12T10:46:33.215233Z",
     "iopub.status.idle": "2024-12-12T10:46:33.262927Z",
     "shell.execute_reply": "2024-12-12T10:46:33.262206Z"
    },
    "papermill": {
     "duration": 0.07434,
     "end_time": "2024-12-12T10:46:33.264743",
     "exception": false,
     "start_time": "2024-12-12T10:46:33.190403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "# query_embeddings.shape, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023b8f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:46:33.315814Z",
     "iopub.status.busy": "2024-12-12T10:46:33.315505Z",
     "iopub.status.idle": "2024-12-12T10:50:03.403761Z",
     "shell.execute_reply": "2024-12-12T10:50:03.402437Z"
    },
    "papermill": {
     "duration": 210.115218,
     "end_time": "2024-12-12T10:50:03.405868",
     "exception": false,
     "start_time": "2024-12-12T10:46:33.290650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "CPU times: user 2.1 s, sys: 584 ms, total: 2.69 s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92489f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:50:03.461995Z",
     "iopub.status.busy": "2024-12-12T10:50:03.461352Z",
     "iopub.status.idle": "2024-12-12T10:50:11.697861Z",
     "shell.execute_reply": "2024-12-12T10:50:11.696997Z"
    },
    "papermill": {
     "duration": 8.268914,
     "end_time": "2024-12-12T10:50:11.700449",
     "exception": false,
     "start_time": "2024-12-12T10:50:03.431535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/lmsys-wheel-files\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\r\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.3.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.77)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3a47d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:50:11.757994Z",
     "iopub.status.busy": "2024-12-12T10:50:11.757687Z",
     "iopub.status.idle": "2024-12-12T10:50:23.440305Z",
     "shell.execute_reply": "2024-12-12T10:50:23.439264Z"
    },
    "papermill": {
     "duration": 11.711193,
     "end_time": "2024-12-12T10:50:23.442353",
     "exception": false,
     "start_time": "2024-12-12T10:50:11.731160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e14b36d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:50:23.495260Z",
     "iopub.status.busy": "2024-12-12T10:50:23.494978Z",
     "iopub.status.idle": "2024-12-12T10:50:23.503172Z",
     "shell.execute_reply": "2024-12-12T10:50:23.502393Z"
    },
    "papermill": {
     "duration": 0.036327,
     "end_time": "2024-12-12T10:50:23.504794",
     "exception": false,
     "start_time": "2024-12-12T10:50:23.468467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run2.py\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import json\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel,BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def batch_to_device(batch, target_device):\n",
    "    \"\"\"\n",
    "    send a pytorch batch to a device (CPU/GPU)\n",
    "    \"\"\"\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                    attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "    pids = list(df['order_index'].values)\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            # outputs = model.model(**features)\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "    \n",
    "    # sentence_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    # result = {pids[i]: em for i, em in enumerate(sentence_embeddings)}\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "    \n",
    "path_prefix = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "# model_path = \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\"\n",
    "device='cuda:0'\n",
    "VALID = False\n",
    "model_path = \"/kaggle/input/qwen2.5-14/pytorch/default/1\"\n",
    "\n",
    "lora_path='/kaggle/input/qwen14b-it-lora/lora_weights/adapter.bin'\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model = AutoModel.from_pretrained(model_path, \n",
    "                                  quantization_config=bnb_config, \n",
    "                                  device_map=\"auto\",\n",
    "                                  trust_remote_code=True)\n",
    "\n",
    "if lora_path:\n",
    "    print(\"loading lora\")\n",
    "    config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    d = torch.load(lora_path, map_location=model.device)\n",
    "    model.load_state_dict(d, strict=False)\n",
    "    model = model.merge_and_unload()\n",
    "model = model.eval()\n",
    "# model = model.to(device)\n",
    "task_description = 'Given a math question with correct answer and a misconcepted incorrect answer, retrieve the most accurate misconception for the incorrect answer.'\n",
    "if VALID:\n",
    "    # tra = pd.read_parquet(\"/kaggle/input/val-parquet/v1_val.parquet\")\n",
    "    tra = pd.read_csv(f\"{path_prefix}/train.csv\").sample(10, random_state=2025, ignore_index=True)\n",
    "    print(tra.shape)\n",
    "else:\n",
    "    tra = pd.read_csv(f\"{path_prefix}/test.csv\")\n",
    "    print(tra.shape)\n",
    "misconception_mapping = pd.read_csv(f\"{path_prefix}/misconception_mapping.csv\")\n",
    "# if tra.shape[0]<10:\n",
    "#     misconception_mapping = misconception_mapping.sample(n=5,random_state=2023)\n",
    "if VALID:\n",
    "    train_data = []\n",
    "    for _,row in tra.iterrows():\n",
    "        for c in ['A','B','C','D']:\n",
    "            if str(row[f\"Misconception{c}Id\"])!=\"nan\":\n",
    "                # print(row[f\"Misconception{c}Id\"])\n",
    "                real_answer_id = row['CorrectAnswer']\n",
    "                real_text = row[f'Answer{real_answer_id}Text']\n",
    "                query_text = f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {real_text}\\n### Misconcepte Incorrect answer: {row[f'Answer{c}Text']}\"\n",
    "                row['query_text'] = get_detailed_instruct(task_description,query_text)\n",
    "                row['answer_id'] = int(row[f\"Misconception{c}Id\"])\n",
    "                train_data.append(copy.deepcopy(row))\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df['order_index'] = list(range(len(train_df)))\n",
    "else:\n",
    "    train_data = []\n",
    "    for _,row in tra.iterrows():\n",
    "        for c in ['A','B','C','D']:\n",
    "            if c ==row['CorrectAnswer']:\n",
    "                continue\n",
    "            if f'Answer{c}Text' not in row:\n",
    "                continue\n",
    "            real_answer_id = row['CorrectAnswer']\n",
    "            real_text = row[f'Answer{real_answer_id}Text']\n",
    "            query_text = f\"### SubjectName: {row['SubjectName']}\\n### ConstructName: {row['ConstructName']}\\n### Question: {row['QuestionText']}\\n### Correct Answer: {real_text}\\n### Misconcepte Incorrect answer: {row[f'Answer{c}Text']}\"\n",
    "            row['query_text'] = get_detailed_instruct(task_description,query_text)\n",
    "            row['answer_name'] = c\n",
    "            train_data.append(copy.deepcopy(row))\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    train_df['order_index'] = list(range(len(train_df)))\n",
    "train_embeddings = inference(train_df, model, tokenizer, device)\n",
    "misconception_mapping['query_text'] = misconception_mapping['MisconceptionName']\n",
    "misconception_mapping['order_index'] = misconception_mapping['MisconceptionId']\n",
    "doc_embeddings = inference(misconception_mapping, model, tokenizer, device)\n",
    "import pickle\n",
    "with open('qwen_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings, f)\n",
    "with open('qwen_misconception.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f94a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T10:50:23.555554Z",
     "iopub.status.busy": "2024-12-12T10:50:23.555058Z",
     "iopub.status.idle": "2024-12-12T11:06:37.681454Z",
     "shell.execute_reply": "2024-12-12T11:06:37.680615Z"
    },
    "papermill": {
     "duration": 974.154152,
     "end_time": "2024-12-12T11:06:37.683801",
     "exception": false,
     "start_time": "2024-12-12T10:50:23.529649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [02:09<00:00, 16.25s/it]\r\n",
      "loading lora\r\n",
      "(3, 11)\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:20<00:00, 20.26s/it]\r\n",
      "Batches: 100%|████████████████████████████████| 162/162 [12:07<00:00,  4.49s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python run2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e54ddef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:37.751414Z",
     "iopub.status.busy": "2024-12-12T11:06:37.751108Z",
     "iopub.status.idle": "2024-12-12T11:06:37.831736Z",
     "shell.execute_reply": "2024-12-12T11:06:37.830347Z"
    },
    "papermill": {
     "duration": 0.116513,
     "end_time": "2024-12-12T11:06:37.833727",
     "exception": false,
     "start_time": "2024-12-12T11:06:37.717214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/kaggle/working/qwen_embeddings.pkl', 'rb') as f:\n",
    "    q_embeddings = pickle.load(f)\n",
    "import pickle\n",
    "\n",
    "with open('/kaggle/working/qwen_misconception.pkl', 'rb') as f:\n",
    "    q_misconception_mapping = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f28f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:37.901262Z",
     "iopub.status.busy": "2024-12-12T11:06:37.900712Z",
     "iopub.status.idle": "2024-12-12T11:06:37.921226Z",
     "shell.execute_reply": "2024-12-12T11:06:37.920421Z"
    },
    "papermill": {
     "duration": 0.056094,
     "end_time": "2024-12-12T11:06:37.923104",
     "exception": false,
     "start_time": "2024-12-12T11:06:37.867010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_data=query_embeddings*0.6+q_embeddings*0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e1863d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:37.992921Z",
     "iopub.status.busy": "2024-12-12T11:06:37.992666Z",
     "iopub.status.idle": "2024-12-12T11:06:38.100093Z",
     "shell.execute_reply": "2024-12-12T11:06:38.099298Z"
    },
    "papermill": {
     "duration": 0.142769,
     "end_time": "2024-12-12T11:06:38.102117",
     "exception": false,
     "start_time": "2024-12-12T11:06:37.959348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc=doc_embeddings*0.7+q_misconception_mapping*0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4862b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:38.169971Z",
     "iopub.status.busy": "2024-12-12T11:06:38.169306Z",
     "iopub.status.idle": "2024-12-12T11:06:38.228510Z",
     "shell.execute_reply": "2024-12-12T11:06:38.227623Z"
    },
    "papermill": {
     "duration": 0.095384,
     "end_time": "2024-12-12T11:06:38.230771",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.135387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = embeddings_data @ doc.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(scores,1, descending=True)[:,:25].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17a60b",
   "metadata": {
    "papermill": {
     "duration": 0.032148,
     "end_time": "2024-12-12T11:06:38.299966",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.267818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Eval (testing purpose only)\n",
    "This will not be ran in real submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8eea3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:38.366808Z",
     "iopub.status.busy": "2024-12-12T11:06:38.366487Z",
     "iopub.status.idle": "2024-12-12T11:06:38.371522Z",
     "shell.execute_reply": "2024-12-12T11:06:38.370736Z"
    },
    "papermill": {
     "duration": 0.041059,
     "end_time": "2024-12-12T11:06:38.373217",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.332158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# def compute_metrics(q_embeds: torch.Tensor, d_embeds: torch.Tensor, target_ids: List[int]):\n",
    "#     \"\"\"\n",
    "#     Compute MAP@25 and Recall@100 metrics.\n",
    "    \n",
    "#     Args:\n",
    "#         q_embeds (torch.Tensor): Query embeddings of shape (M, dim), where M is the number of queries.\n",
    "#         d_embeds (torch.Tensor): Document embeddings of shape (N, dim), where N is the number of documents.\n",
    "#         target_ids (List[int]): List of target document indices (length M, one target index per query).\n",
    "        \n",
    "#     Returns:\n",
    "#         None: Prints MAP@25 and Recall@100.\n",
    "#     \"\"\"\n",
    "#     # Compute similarity scores\n",
    "#     scores = q_embeds @ d_embeds.T  # Shape: (M, N)\n",
    "\n",
    "#     # Initialize variables for metrics\n",
    "#     avg_precisions = []  # To store average precision for each query\n",
    "#     recall_counts = []   # To store recall@100 counts for each query\n",
    "\n",
    "#     # Compute metrics for each query\n",
    "#     for i, target_id in enumerate(target_ids):\n",
    "#         # Sort document indices by score in descending order\n",
    "#         sorted_indices = torch.argsort(scores[i], descending=True)\n",
    "\n",
    "#         # Compute precision@k and recall@100\n",
    "#         relevant_docs = (sorted_indices[:100] == target_id).nonzero(as_tuple=True)[0]  # Find rank within top 100\n",
    "#         recall_count = 1 if len(relevant_docs) > 0 else 0  # Check if target is in the top 100\n",
    "#         recall_counts.append(recall_count)\n",
    "\n",
    "#         # Compute average precision for top 25 (MAP@25)\n",
    "#         precision_at_k = 0.0\n",
    "#         num_relevant = 0\n",
    "#         for rank, idx in enumerate(sorted_indices[:25]):\n",
    "#             if idx == target_id:\n",
    "#                 num_relevant += 1\n",
    "#                 precision_at_k += num_relevant / (rank + 1)\n",
    "#         avg_precisions.append(precision_at_k / 1 if num_relevant > 0 else 0)\n",
    "\n",
    "#     # Calculate metrics\n",
    "#     map25 = sum(avg_precisions) / len(avg_precisions)\n",
    "#     recall100 = sum(recall_counts) / len(recall_counts)\n",
    "\n",
    "#     # Print results\n",
    "#     print(f\"MAP@25: {map25:.4f}\")\n",
    "#     print(f\"Recall@100: {recall100:.4f}\")\n",
    "# if not IS_SUBMISSION:\n",
    "#     compute_metrics(query_embeddings, doc_embeddings, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12b95dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:38.439261Z",
     "iopub.status.busy": "2024-12-12T11:06:38.439015Z",
     "iopub.status.idle": "2024-12-12T11:06:38.827077Z",
     "shell.execute_reply": "2024-12-12T11:06:38.826364Z"
    },
    "papermill": {
     "duration": 0.423544,
     "end_time": "2024-12-12T11:06:38.829003",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.405459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "IS_SUBMISSION =True\n",
    "df_ret = df_test.copy()\n",
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "            continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639fe395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:38.898020Z",
     "iopub.status.busy": "2024-12-12T11:06:38.897355Z",
     "iopub.status.idle": "2024-12-12T11:06:38.910769Z",
     "shell.execute_reply": "2024-12-12T11:06:38.910121Z"
    },
    "papermill": {
     "duration": 0.048152,
     "end_time": "2024-12-12T11:06:38.912305",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.864153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"s1.csv\", index=False)\n",
    "\n",
    "s=pd.read_csv('s1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8ca441c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:38.978069Z",
     "iopub.status.busy": "2024-12-12T11:06:38.977842Z",
     "iopub.status.idle": "2024-12-12T11:06:38.981227Z",
     "shell.execute_reply": "2024-12-12T11:06:38.980395Z"
    },
    "papermill": {
     "duration": 0.038129,
     "end_time": "2024-12-12T11:06:38.982774",
     "exception": false,
     "start_time": "2024-12-12T11:06:38.944645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.read_csv('s1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23cf0899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:39.048432Z",
     "iopub.status.busy": "2024-12-12T11:06:39.047835Z",
     "iopub.status.idle": "2024-12-12T11:06:39.052436Z",
     "shell.execute_reply": "2024-12-12T11:06:39.051815Z"
    },
    "papermill": {
     "duration": 0.039128,
     "end_time": "2024-12-12T11:06:39.054164",
     "exception": false,
     "start_time": "2024-12-12T11:06:39.015036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n1=s[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n",
    "n1=[i for i in n1]\n",
    "np.save(\"n1.npy\", np.array(n1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "280f0397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:39.120777Z",
     "iopub.status.busy": "2024-12-12T11:06:39.120520Z",
     "iopub.status.idle": "2024-12-12T11:06:39.194816Z",
     "shell.execute_reply": "2024-12-12T11:06:39.194188Z"
    },
    "papermill": {
     "duration": 0.108501,
     "end_time": "2024-12-12T11:06:39.196392",
     "exception": false,
     "start_time": "2024-12-12T11:06:39.087891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "for idx, row in full_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "            \n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text =f\"###question###:{row['SubjectName']}-{row['ConstructName']}-{row['QuestionText']}\\n###Correct Answer###:{correct_answer}\\n###Misconcepte Incorrect answer###:{option}.{row[f'Answer{option}Text']}\"\n",
    "\n",
    "        rows.append({\"query_text\": query_text, \n",
    "                     \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                     \"ConstructName\": row.ConstructName,\n",
    "                     \"SubjectName\": row.SubjectName,\n",
    "                     \"QuestionText\": row.QuestionText,\n",
    "                     \"correct_answer\": correct_answer,\n",
    "                     \"incorrect_answer\": row[f\"Answer{option}Text\"]\n",
    "                     })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_parquet(\"data.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e37bd92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:39.263827Z",
     "iopub.status.busy": "2024-12-12T11:06:39.263236Z",
     "iopub.status.idle": "2024-12-12T11:06:39.269679Z",
     "shell.execute_reply": "2024-12-12T11:06:39.268898Z"
    },
    "papermill": {
     "duration": 0.04139,
     "end_time": "2024-12-12T11:06:39.271512",
     "exception": false,
     "start_time": "2024-12-12T11:06:39.230122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "# just directly give your answers.\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "df = pd.read_parquet(\"data.parquet\")\n",
    "indices = np.load(\"n1.npy\")\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "survivors = indices[:, -1:]\n",
    "\n",
    "for i in range(3):\n",
    "    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "    \n",
    "    df[\"retrieval\"] = get_candidates(c_indices)\n",
    "    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "    \n",
    "    print(\"Example:\")\n",
    "    print(df[\"text\"].values[0])\n",
    "    print()\n",
    "    \n",
    "    responses = llm.generate(\n",
    "        df[\"text\"].values,\n",
    "        vllm.SamplingParams(\n",
    "            n=1,  # Number of output sequences to return for each prompt.\n",
    "            top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "            temperature=0,  # randomness of the sampling\n",
    "            seed=777, # Seed for reprodicibility\n",
    "            skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "            max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "        ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "    \n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    df[\"response\"] = responses\n",
    "    \n",
    "    \n",
    "    llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    \n",
    "    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "results1 = []\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    ix = indices[i]\n",
    "    llm_choice = survivors[i, 0]\n",
    "    \n",
    "    results1.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "df[\"MisconceptionId\"] = results1\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be7ace93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:06:39.338413Z",
     "iopub.status.busy": "2024-12-12T11:06:39.338159Z",
     "iopub.status.idle": "2024-12-12T11:09:01.978510Z",
     "shell.execute_reply": "2024-12-12T11:09:01.977636Z"
    },
    "papermill": {
     "duration": 142.675889,
     "end_time": "2024-12-12T11:09:01.980660",
     "exception": false,
     "start_time": "2024-12-12T11:06:39.304771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 11:06:44 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-12 11:06:44 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-12 11:06:44 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-12 11:06:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-12 11:06:45 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:45 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:45 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-12 11:06:45 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:47 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-12 11:06:47 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:47 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:47 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 11:06:47 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-12 11:06:47 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 11:06:55 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:55 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-12 11:06:55 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ef09c60e410>, local_subscribe_port=58941, local_sync_port=37601, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-12 11:06:55 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:55 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:55 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:06:55 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-12 11:06:55 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-12 11:06:55 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:16<01:05, 16.28s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:34<00:51, 17.19s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:53<00:36, 18.34s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:11<00:18, 18.24s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:30<00:00, 18.37s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:30<00:00, 18.10s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=231)\u001b[0;0m INFO 12-12 11:08:26 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:08:26 model_runner.py:692] Loading model weights took 9.0934 GB\r\n",
      "INFO 12-12 11:08:36 distributed_gpu_executor.py:56] # GPU blocks: 795, # CPU blocks: 2048\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Confuses the order of operations, believes addition comes before division\r\n",
      "2. Performs addition ahead of division\r\n",
      "3. Does not include brackets when required\r\n",
      "4. Does not interpret the correct order of operations from a worded problem\r\n",
      "5. Believes that the order of a worded calculation should be changed to follow BIDMAS \r\n",
      "6. Confuses the order of operations, believes subtraction comes before multiplication \r\n",
      "7. Does not understand the effect of consecutive operations\r\n",
      "8. Does not realise addition is commutative\r\n",
      "9. Performs subtraction in wrong order<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:06<00:00,  1.45it/s, est. speed input: 468.02\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Carries out operations from left to right regardless of priority order, unless brackets are used\r\n",
      "2. Confuses the order of operations, believes addition comes before multiplication \r\n",
      "3. Answers order of operations questions with brackets as if the brackets are not there\r\n",
      "4. May have made a calculation error using the order of operations\r\n",
      "5. Performs subtraction right to left if priority order means doing a calculation to the right first\r\n",
      "6. Believes addition comes before indices, in orders of operation\r\n",
      "7. Subtracts instead of adds\r\n",
      "8. Has not realised that the answer may be changed by the insertion of brackets\r\n",
      "9. Confuses the order of operations, believes subtraction comes before multiplication<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.54it/s, est. speed input: 495.22\r\n",
      "Example:\r\n",
      "<|im_start|>system\r\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Here is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\r\n",
      "Question: \\[\r\n",
      "3 \\times 2+4-5\r\n",
      "\\]\r\n",
      "Where do the brackets need to go to make the answer equal 13 ?\r\n",
      "Correct Answer: 3 \\times(2+4)-5 \r\n",
      "Incorrect Answer: 3 \\times 2+(4-5) \r\n",
      "\r\n",
      "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\r\n",
      "Answer concisely what misconception it is to lead to getting the incorrect answer.\r\n",
      "Pick the correct misconception number from the below:\r\n",
      "\r\n",
      "1. Carries out operations from right to left regardless of priority order\r\n",
      "2. Inserts brackets but not changed order of operation\r\n",
      "3. Carries out operations from left to right regardless of priority order\r\n",
      "4. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\r\n",
      "5. Performs addition ahead of multiplication\r\n",
      "6. Performs addition ahead of subtraction\r\n",
      "7. Performs addition ahead of any other operation\r\n",
      "8. Believes order of operations does not affect the answer to a calculation\r\n",
      "9. Answers order of operations questions with brackets as if the brackets are not there<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "\r\n",
      "Processed prompts: 100%|█| 9/9 [00:05<00:00,  1.51it/s, est. speed input: 495.90\r\n",
      "[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4504de4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T11:09:02.050192Z",
     "iopub.status.busy": "2024-12-12T11:09:02.049881Z",
     "iopub.status.idle": "2024-12-12T11:09:02.065428Z",
     "shell.execute_reply": "2024-12-12T11:09:02.064650Z"
    },
    "papermill": {
     "duration": 0.051928,
     "end_time": "2024-12-12T11:09:02.067077",
     "exception": false,
     "start_time": "2024-12-12T11:09:02.015149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>1345 706 1507 2306 328 2181 1516 2532 1005 167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>1345 1507 2306 706 1005 2488 2532 2181 328 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>315 2532 1345 1507 328 1516 2488 1005 2306 167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>891 1755 167 418 2142 2068 979 1535 1871 1593 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>891 167 979 1755 1593 1871 2142 143 2068 418 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>891 1755 167 418 2142 2068 113 1535 979 1593 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 2439 1306 1059 2551 1098 1677 1200 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 2439 1059 2551 1306 1098 365 1677 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1287 1073 2439 1059 2471 2551 397 365 1923 167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  1345 706 1507 2306 328 2181 1516 2532 1005 167...\n",
       "1            1869_C  1345 1507 2306 706 1005 2488 2532 2181 328 251...\n",
       "2            1869_D  315 2532 1345 1507 328 1516 2488 1005 2306 167...\n",
       "3            1870_A  891 1755 167 418 2142 2068 979 1535 1871 1593 ...\n",
       "4            1870_B  891 167 979 1755 1593 1871 2142 143 2068 418 5...\n",
       "5            1870_C  891 1755 167 418 2142 2068 113 1535 979 1593 1...\n",
       "6            1871_A  1287 1073 2439 1306 1059 2551 1098 1677 1200 3...\n",
       "7            1871_C  1287 1073 2439 1059 2551 1306 1098 365 1677 16...\n",
       "8            1871_D  1287 1073 2439 1059 2471 2551 397 365 1923 167..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6135443,
     "sourceId": 9972502,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10171817,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 210418668,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 165390,
     "modelInstanceId": 142811,
     "sourceId": 167864,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 180936,
     "modelInstanceId": 158570,
     "sourceId": 185986,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1738.283434,
   "end_time": "2024-12-12T11:09:02.820856",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T10:40:04.537422",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
